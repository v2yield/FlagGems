# Generated by KernelGen v1.0
# Source: Triton
# Performance: 2.6x vs Native on A100
# License: Apache-2.0

import os
import sys
import time

import pytest
import torch

import flag_gems

# Add parent directory to path to import flag_gems
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "../../.."))
try:
    from tests.accuracy_utils import FLOAT_DTYPES, gems_assert_close, to_reference
except ImportError:
    # Fallback values when running outside pytest
    FLOAT_DTYPES = [torch.float16, torch.float32, torch.bfloat16]

    def gems_assert_close(res, ref, dtype, **kwargs):
        # Simple fallback comparison
        torch.testing.assert_close(res, ref, **kwargs)

    def to_reference(x, upcast=False):
        # Simple fallback - move to CPU
        return x.to("cpu")


def rmsnorm(
    input_tensor: torch.Tensor, weight: torch.Tensor, eps: float = 1e-6
) -> torch.Tensor:
    orig_dtype = input_tensor.dtype
    # Use float32 for computation when possible for numerical stability
    compute_dtype = (
        torch.float32
        if input_tensor.dtype in (torch.float16, torch.bfloat16)
        else input_tensor.dtype
    )

    x = input_tensor.to(compute_dtype)
    w = weight.to(compute_dtype)

    # Compute inverse RMS over the last dimension
    inv_rms = torch.rsqrt(torch.mean(x * x, dim=-1, keepdim=True) + eps)
    y = x * inv_rms

    # Apply learnable scaling
    y = y * w

    return y.to(orig_dtype)


# Test shapes for normalization operations
NMSNORM_SHAPES = [
    (16, 512),  # Typical transformer hidden size
    (32, 1024),  # Larger hidden size
    (8, 64, 768),  # Batch, seq_len, hidden_size
    (4, 128, 1024),  # Larger batch and hidden
    (1, 2048),  # Single sample, large hidden
]


@pytest.mark.rmsnorm_accuracy
@pytest.mark.parametrize("shape", NMSNORM_SHAPES)
@pytest.mark.parametrize("dtype", FLOAT_DTYPES)
@pytest.mark.parametrize("eps", [1e-5, 1e-6])
def test_nmsnorm_accuracy(shape, dtype, eps):
    inp = torch.randn(shape, dtype=dtype, device=flag_gems.device)
    ref_inp = to_reference(inp, True)

    # Test with weight
    weight = torch.randn(shape[-1:], dtype=dtype, device=flag_gems.device)
    ref_weight = to_reference(weight, True)

    # Reference computation - adjust based on rmsnorm definition
    # This is a placeholder - replace with actual reference implementation
    ref_out = rmsnorm(ref_inp, weight=ref_weight, eps=eps)

    # FlagGems computation
    with flag_gems.use_gems():
        res_out = flag_gems.experimental_ops.rmsnorm(inp, weight=weight, eps=eps)

    # Move result to CPU for comparison
    res_out_cpu = res_out.cpu()
    gems_assert_close(res_out_cpu, ref_out, dtype, equal_nan=True)


@pytest.mark.rmsnorm_performance
@pytest.mark.parametrize("shape", NMSNORM_SHAPES)
@pytest.mark.parametrize("dtype", FLOAT_DTYPES)
def test_nmsnorm_performace(shape, dtype):
    inp = torch.randn(shape, dtype=dtype, device=flag_gems.device)
    ref_inp = to_reference(inp, True)

    # Test with weight
    weight = torch.randn(shape[-1:], dtype=dtype, device=flag_gems.device)
    ref_weight = to_reference(weight, True)

    # Warmup
    for _ in range(10):
        _ = flag_gems.experimental_ops.rmsnorm(inp, weight)

    torch.cuda.synchronize()

    # Benchmark
    start_time = time.time()
    for _ in range(100):
        _ = flag_gems.experimental_ops.rmsnorm(inp, weight)
    torch.cuda.synchronize()
    end_time = time.time()

    gems_time = (end_time - start_time) / 100

    # PyTorch baseline
    start_time = time.time()
    for _ in range(100):
        _ = rmsnorm(ref_inp, ref_weight)
    torch.cuda.synchronize()
    end_time = time.time()

    torch_time = (end_time - start_time) / 100
    speedup = torch_time / gems_time

    print(f"rmsnorm {shape} {dtype}:")
    print(f"  FlagGems: {gems_time * 1000:.3f}ms")
    print(f"  PyTorch: {torch_time * 1000:.3f}ms")
    print(f"  Speedup: {speedup:.2f}x")
