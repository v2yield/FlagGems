# Generated by KernelGen v1.0
# Source: Triton
# Performance: 2.6x vs Native on A100
# License: Apache-2.0


import torch
import triton
import triton.language as tl


@triton.jit
def rmsnorm(
    input_ptr,  # *Pointer* to the input tensor flattened to 2D [M, N]
    weight_ptr,  # *Pointer* to the weight tensor [N]
    output_ptr,  # *Pointer* to the output tensor flattened to 2D [M, N]
    M,  # Number of rows (prod of leading dims)
    N,  # Hidden size (last dimension)
    eps,  # Epsilon for numerical stability
    BLOCK_SIZE: tl.constexpr,
):
    pid = tl.program_id(axis=0)  # each program handles one row
    row_start = pid * N

    # First pass: compute sum of squares across the row
    col_start = 0
    acc = tl.zeros([1], dtype=tl.float32)
    while col_start < N:
        offs = col_start + tl.arange(0, BLOCK_SIZE)
        mask = offs < N
        x = tl.load(input_ptr + row_start + offs, mask=mask, other=0.0)
        x = x.to(tl.float32)
        acc += tl.sum(x * x, axis=0)
        col_start += BLOCK_SIZE

    mean_sq = acc / N
    inv_rms = 1.0 / tl.sqrt(mean_sq + eps)

    # Second pass: normalize and scale
    col_start = 0
    while col_start < N:
        offs = col_start + tl.arange(0, BLOCK_SIZE)
        mask = offs < N
        x = tl.load(input_ptr + row_start + offs, mask=mask, other=0.0).to(tl.float32)
        w = tl.load(weight_ptr + offs, mask=mask, other=0.0).to(tl.float32)
        y = x * inv_rms * w
        tl.store(output_ptr + row_start + offs, y, mask=mask)
        col_start += BLOCK_SIZE


# Keep a handle to the Triton kernel before defining the Python wrapper with the same name
rmsnorm_kernel = rmsnorm


def rmsnorm(input_tensor: torch.Tensor, weight: torch.Tensor, eps: float = 1e-6):
    assert input_tensor.is_cuda and weight.is_cuda, "Tensors must be on CUDA device."
    assert (
        input_tensor.shape[-1] == weight.numel()
    ), "weight must have shape (hidden_size,) matching the last dim of input_tensor."

    x = input_tensor
    w = weight
    hidden_size = x.shape[-1]
    M = x.numel() // hidden_size
    N = hidden_size

    # Ensure contiguous memory along the last dimension
    x_2d = x.contiguous().view(M, N)
    out = torch.empty_like(x)
    out_2d = out.view(M, N)
    w_c = w.contiguous()

    # Choose a reasonable BLOCK_SIZE (power-of-two up to 4096)
    def next_pow2(v: int) -> int:
        return 1 if v <= 1 else 1 << ((v - 1).bit_length())

    BLOCK_SIZE = min(4096, max(128, next_pow2(N)))
    num_warps = 4 if BLOCK_SIZE <= 1024 else 8

    grid = lambda meta: (M,)

    rmsnorm_kernel[grid](
        x_2d,
        w_c,
        out_2d,
        M,
        N,
        eps,
        BLOCK_SIZE=BLOCK_SIZE,
        num_warps=num_warps,
    )
    return out
