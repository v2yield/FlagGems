[English](./README.md)


## ä»‹ç»

FlagGems æ˜¯ä¸€ä¸ªä½¿ç”¨ OpenAI æ¨å‡ºçš„[Triton ç¼–ç¨‹è¯­è¨€](https://github.com/openai/triton)å®ç°çš„é«˜æ€§èƒ½é€šç”¨ç®—å­åº“ï¼Œæ—¨åœ¨ä¸ºå¤§è¯­è¨€æ¨¡å‹æä¾›ä¸€ç³»åˆ—å¯åº”ç”¨äº PyTorch æ¡†æ¶çš„ç®—å­ï¼ŒåŠ é€Ÿæ¨¡å‹é¢å‘å¤šç§åç«¯å¹³å°çš„æ¨ç†ä¸è®­ç»ƒã€‚

FlagGems é€šè¿‡å¯¹ PyTorch çš„åç«¯ aten ç®—å­è¿›è¡Œè¦†ç›–é‡å†™ï¼Œå®ç°ç®—å­åº“çš„æ— ç¼æ›¿æ¢ï¼Œä¸€æ–¹é¢æ¨¡å‹å¼€å‘è€…èƒ½å¤Ÿåœ¨æ— éœ€ä¿®æ”¹åº•å±‚ API çš„æƒ…å†µä¸‹å¹³ç¨³åœ°åˆ‡æ¢åˆ° triton ç®—å­åº“ï¼Œä½¿ç”¨å…¶ç†Ÿæ‚‰çš„ PyTorch API åŒæ—¶äº«å—æ–°ç¡¬ä»¶å¸¦æ¥çš„åŠ é€Ÿèƒ½åŠ›ï¼Œå¦ä¸€æ–¹é¢å¯¹ kernel å¼€å‘è€…è€Œè¨€ï¼ŒTriton è¯­è¨€æä¾›äº†æ›´å¥½çš„å¯è¯»æ€§å’Œæ˜“ç”¨æ€§ï¼Œå¯åª²ç¾ CUDA çš„æ€§èƒ½ï¼Œå› æ­¤å¼€å‘è€…åªéœ€ä»˜å‡ºè¾ƒä½çš„å­¦ä¹ æˆæœ¬ï¼Œå³å¯å‚ä¸ FlagGems çš„ç®—å­å¼€å‘ä¸å»ºè®¾ã€‚

æˆ‘ä»¬ä¸º FlagGems åˆ›å»ºäº†å¾®ä¿¡ç¾¤ã€‚æ‰«æäºŒç»´ç å³å¯åŠ å…¥ç¾¤èŠï¼ç¬¬ä¸€æ—¶é—´äº†è§£æˆ‘ä»¬çš„åŠ¨æ€å’Œä¿¡æ¯å’Œæ–°ç‰ˆæœ¬å‘å¸ƒï¼Œæˆ–è€…æœ‰ä»»ä½•é—®é¢˜æˆ–æƒ³æ³•ï¼Œè¯·ç«‹å³åŠ å…¥æˆ‘ä»¬ï¼

<p align="center">
<img width="204" height="180" alt="å¼€æºå°åŠ©æ‰‹" src="https://github.com/user-attachments/assets/4e9a8566-c91e-4120-a011-6b5577c1a53d" />
</p>

## ç‰¹æ€§

- æ”¯æŒçš„ç®—å­æ•°é‡è§„æ¨¡è¾ƒå¤§
- éƒ¨åˆ†ç®—å­å·²ç»è¿‡æ·±åº¦æ€§èƒ½è°ƒä¼˜
- å¯ç›´æ¥åœ¨ Eager æ¨¡å¼ä¸‹ä½¿ç”¨, æ— éœ€é€šè¿‡ `torch.compile`
- Pointwise è‡ªåŠ¨ä»£ç ç”Ÿæˆï¼Œçµæ´»æ”¯æŒå¤šç§è¾“å…¥ç±»å‹å’Œå†…å­˜æ’å¸ƒ
- Triton kernel è°ƒç”¨ä¼˜åŒ–
- çµæ´»çš„å¤šåç«¯æ”¯æŒæœºåˆ¶
- ä»£ç åº“å·²é›†æˆåä½™ç§åç«¯
- C++ Triton å‡½æ•°æ´¾å‘ (å¼€å‘ä¸­)

## æ›´å¤šç‰¹æ€§ç»†èŠ‚

### å¤šåç«¯ç¡¬ä»¶æ”¯æŒ

FlagGems æ”¯æŒæ›´å¤šçš„ç¡¬ä»¶å¹³å°å¹¶ä¸”åœ¨ä¸åŒç¡¬ä»¶ä¸Šè¿›è¡Œäº†å……åˆ†çš„æµ‹è¯•ã€‚

### è‡ªåŠ¨ä»£ç ç”Ÿæˆ

FlagGems æä¾›äº†ä¸€å¥—è‡ªåŠ¨ä»£ç ç”Ÿæˆçš„æœºåˆ¶ï¼Œå¼€å‘è€…å¯ä»¥ä½¿ç”¨å®ƒæ¥ä¾¿æ·åœ°ç”Ÿæˆ pointwise ç±»å‹çš„å•ç®—å­ä¸èåˆç®—å­ã€‚è‡ªåŠ¨ä»£ç ç”Ÿæˆå¯ä»¥å¤„ç†å¸¸è§„çš„å¯¹ä½è®¡ç®—ã€éå¼ é‡å‚æ•°ã€æŒ‡å®šè¾“å‡ºç±»å‹ç­‰å¤šç§éœ€æ±‚ã€‚è¯¦ç»†ä¿¡æ¯å‚è€ƒ [pointwise_dynamic](docs/pointwise_dynamic.md)

### LibEntry

FlagGems æ„é€ äº† `LibEntry` ç‹¬ç«‹ç»´æŠ¤ kernel cache, ç»•è¿‡ `Autotuner`ã€`Heuristics` å’Œ `JitFunction` çš„ runtime, ä½¿ç”¨æ—¶ä»…éœ€åœ¨ triton kernel å‰è£…é¥°å³å¯ã€‚`LibEntry` æ”¯æŒ `Autotuner`ã€`Heuristics`ã€`JitFunction` çš„ç›´æ¥åŒ…è£…ï¼Œä¸å½±å“è°ƒå‚åŠŸèƒ½çš„æ­£å¸¸ä½¿ç”¨ï¼Œä½†æ˜¯æ— éœ€ç»è¿‡è¿è¡Œæ—¶ç±»å‹çš„åµŒå¥—è°ƒç”¨ï¼ŒèŠ‚çœäº†é‡å¤çš„å‚æ•°å¤„ç†ï¼Œæ— éœ€ç»‘å®šå’Œç±»å‹åŒ…è£…ï¼Œç®€åŒ–äº† cache key æ ¼å¼ï¼Œå‡å°‘ä¸å¿…è¦çš„é”®å€¼è®¡ç®—ã€‚

### C++ è¿è¡Œæ—¶

FlagGems å¯ä»¥ä½œä¸ºçº¯ Python åŒ…å®‰è£…ï¼Œä¹Ÿå¯ä»¥ä½œä¸ºå¸¦æœ‰ C++ æ‰©å±•çš„åŒ…å®‰è£…ã€‚C++ è¿è¡Œæ—¶æ—¨åœ¨è§£å†³ python è¿è¡Œæ—¶å¼€é”€æ˜‚è´µçš„é—®é¢˜, æé«˜æ•´ä¸ªç«¯åˆ°ç«¯çš„æ€§èƒ½ã€‚

## æ›´æ–°æ—¥å¿—

### v4.2ï¼ˆå³å°†å‘å¸ƒï¼‰

- è®¡åˆ’æ”¯æŒ 216 ä¸ªç®—å­ï¼Œå¹¶ä¸æœ€æ–°çš„ [Operator List](docs/operator_list.md) ä¿æŒä¸€è‡´
- è®¡åˆ’æ–°å¢ç®—å­ï¼š`tan`ã€`tan_`ã€`baddbmm`ã€`avg_pool2d`ã€`clamp_min`ã€`clamp_min_`ã€`std`ã€`trace`ã€`max_pool2d`ã€`bitwise_left_shift`ã€`bitwise_right_shift`
- åŸæœ‰çš„ `upsample` ç®—å­å°†æ‹†åˆ†ä¸º `upsample_nearest2d` ä¸ `upsample_bicubic2d_aa`

### v4.1

- é¢å‘ RWKV æ¨¡å‹çš„å®šåˆ¶ç‰ˆæœ¬ï¼Œå…±è®¡æ”¯æŒ 204 ä¸ªç®—å­
- åŒ…å«é’ˆå¯¹ RWKV æ¨ç†åŠ é€Ÿåœºæ™¯ä¼˜åŒ–çš„èåˆç®—å­ï¼š`rwkv_mm_sparsity`ã€`rwkv_ka_fusion`
- å·²è¢« RWKV é¡¹ç›®é‡‡ç”¨ï¼Œå…·ä½“è§ [BlinkDL/Albatross:faster_251101](https://github.com/BlinkDL/Albatross/tree/main/faster_251101)

### v4.0

- å…±è®¡æ”¯æŒ 202 ä¸ªç®—å­
- æ–°å¢é€šç”¨ç®—å­ï¼š`addcdiv`ã€`addcmul`ã€`addmv`ã€`addr`ã€`atan`ã€`atan_`ã€`celu`ã€`celu_`ã€`elu_`ã€`exp2`ã€`exp2_`ã€`get_scheduler_metadata`ã€`index_add_`ã€`logspace`ã€`moe_align_block_size`ã€`softplus`ã€`sqrt_`ã€`topk_softmax`
- Triton JIT runtime æ–°å¢æ”¯æŒçš„ç®—å­ï¼š`add`ã€`addmm`ã€`argmax`ã€`bmm`ã€`cat`ã€`contiguous`ã€`embedding`ã€`exponential_`ã€`fill`ã€`flash_attn_varlen_func`ã€`fused_add_rms_norm`ã€`max`ã€`mm`ã€`nonzero`ã€`reshape_and_cache_flash`ã€`rms_norm`ã€`rotary_embedding`ã€`softmax`ã€`sum`ã€`topk`ã€`zeros`

### v3.0

- å…±è®¡æ”¯æŒ 184 ä¸ªç®—å­ï¼ŒåŒ…æ‹¬å¤§æ¨¡å‹æ¨ç†ä½¿ç”¨çš„å®šåˆ¶ç®—å­
- æ”¯æŒæ›´å¤šçš„ç¡¬ä»¶å¹³å°ï¼Œæ–°å¢ Ascendã€AIPU ç­‰
- å…¼å®¹ vllm æ¡†æ¶ï¼ŒDeepSeek æ¨¡å‹æ¨ç†éªŒè¯é€šè¿‡

### v2.1

- æ”¯æŒ Tensor ç±»ç®—å­ï¼šwhere, arange, repeat, masked_fill, tile, unique, index_select, masked_select, ones, ones_like, zeros, zeros_like, full, full_like, flip, pad
- æ”¯æŒç¥ç»ç½‘ç»œç±»ç®—å­ï¼šembedding
- æ”¯æŒåŸºç¡€æ•°å­¦ç®—å­ï¼šallclose, isclose, isfinite, floor_divide, trunc_divide, maximum, minimum
- æ”¯æŒåˆ†å¸ƒç±»ç®—å­ï¼šnormal, uniform\_, exponential\_, multinomial, nonzero, topk, rand, randn, rand_like, randn_like
- æ”¯æŒç§‘å­¦è®¡ç®—ç®—å­ï¼šerf, resolve_conj, resolve_neg

### v2.0

- æ”¯æŒ BLAS ç±»ç®—å­: mv, outer
- æ”¯æŒ pointwise ç±»ç®—å­: bitwise_and, bitwise_not, bitwise_or, cos, clamp, eq, ge, gt, isinf, isnan, le, lt, ne, neg, or, sin, tanh, sigmoid
- æ”¯æŒ reduction ç±»ç®—å­: all, any, amax, argmax, max, min, prod, sum, var_mean, vector_norm, cross_entropy_loss, group_norm, log_softmax, rms_norm
- æ”¯æŒèåˆç®—å­: fused_add_rms_norm, skip_layer_norm, gelu_and_mul, silu_and_mul, apply_rotary_position_embedding

### v1.0

- æ”¯æŒ BLAS ç±»ç®—å­ï¼šaddmm, bmm, mm
- æ”¯æŒ pointwise ç±»ç®—å­ï¼šabs, add, div, dropout, exp, gelu, mul, pow, reciprocal, relu, rsqrt, silu, sub, triu
- æ”¯æŒ reduction ç±»ç®—å­ï¼šcumsum, layernorm, mean, softmax

## å¿«é€Ÿå…¥é—¨

å‚è€ƒæ–‡æ¡£ [å¼€å§‹ä½¿ç”¨](docs/get_start_with_flaggems.md) å¿«é€Ÿå®‰è£…ä½¿ç”¨ flag_gems

## ä½¿ç”¨æ–¹æ³•

FlagGems æ”¯æŒä¸¤ç§å¸¸è§çš„ä½¿ç”¨æ¨¡å¼ï¼šå¯¹ PyTorch ATen ç®—å­æ‰“è¡¥ä¸ï¼ˆæ¨èï¼‰å’Œæ˜¾å¼è°ƒç”¨ FlagGems ç®—å­ã€‚

### (1) å…¨å±€å¯ç”¨ FlagGemsï¼ˆå¯¹ ATen ç®—å­æ‰“è¡¥ä¸ï¼‰

æ‰§è¡Œ `flag_gems.enable()` åï¼Œæ”¯æŒçš„ `torch.*` / `torch.nn.functional.*` è°ƒç”¨å°†ä¼šè‡ªåŠ¨åˆ†å‘ï¼ˆdispatchï¼‰åˆ° FlagGems çš„å®ç°ä¸Šã€‚

```python
import torch
import flag_gems

flag_gems.enable()

x = torch.randn(4096, 4096, device=flag_gems.device, dtype=torch.float16)
y = torch.mm(x, x)
```

å¦‚æœä½ åªæƒ³åœ¨æŸä¸ªä½œç”¨åŸŸå†…ï¼ˆä¾‹å¦‚ç”¨äºåŸºå‡†æµ‹è¯•ï¼‰ä½¿ç”¨ FlagGemsï¼Œè¯·ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼š

```python
import torch
import flag_gems

with flag_gems.use_gems():
    x = torch.randn(4096, 4096, device=flag_gems.device, dtype=torch.float16)
    y = torch.mm(x, x)
```

### (2) æ˜¾å¼è°ƒç”¨ FlagGems ç®—å­
ä½ ä¹Ÿå¯ä»¥ç»•è¿‡ PyTorch çš„åˆ†å‘æœºåˆ¶ï¼Œç›´æ¥ä» flag_gems.ops ä¸­è°ƒç”¨ç®—å­ï¼Œæ­¤æ—¶æ— éœ€è°ƒç”¨ enable()ï¼š

```python
import torch
from flag_gems import ops
import flag_gems

a = torch.randn(1024, 1024, device=flag_gems.device, dtype=torch.float16)
b = torch.randn(1024, 1024, device=flag_gems.device, dtype=torch.float16)
c = ops.mm(a, b)
```
è‹¥è¦äº†è§£æ›´å¤šè¯¦æƒ…å’Œé«˜çº§é€‰é¡¹ï¼ˆä¾‹å¦‚ç¦ç”¨ç‰¹å®šç®—å­ã€è¿è¡Œæ—¶æ—¥å¿—ç­‰ï¼‰ï¼Œè¯·å‚é˜… [`how_to_use_flaggems`](docs/how_to_use_flaggems.md)ã€‚


## æ”¯æŒç®—å­

ç®—å­å°†æŒ‰ç…§æ–‡æ¡£ [OperatorList](docs/operator_list.md) çš„é¡ºåºé€æ­¥å®ç°ã€‚

## æ”¯æŒæ¨¡å‹

- Bert-base-uncased
- Llama-2-7b
- Llava-1.5-7b

## æ”¯æŒå¹³å°

| vendor     | state                  | float16 | float32 | bfloat16 |
| ---------- | ---------------------- | ------- | ------- | -------- |
| aipu       | âœ… ï¼ˆPartial supportï¼‰ | âœ…      | âœ…      | âœ…       |
| ascend     | âœ… ï¼ˆPartial supportï¼‰ | âœ…      | âœ…      | âœ…       |
| cambricon  | âœ…                     | âœ…      | âœ…      | âœ…       |
| hygon      | âœ…                     | âœ…      | âœ…      | âœ…       |
| iluvatar   | âœ…                     | âœ…      | âœ…      | âœ…       |
| kunlunxin  | âœ…                     | âœ…      | âœ…      | âœ…       |
| metax      | âœ…                     | âœ…      | âœ…      | âœ…       |
| mthreads   | âœ…                     | âœ…      | âœ…      | âœ…       |
| nvidia     | âœ…                     | âœ…      | âœ…      | âœ…       |
| arm(cpu)   | ğŸš§                     |         |         |          |
| tsingmicro | ğŸš§                     |         |         |          |

## æ€§èƒ½è¡¨ç°

FlagGems ç›¸æ¯” Torch Eager æ¨¡å¼ä¸‹ ATen ç®—å­åº“çš„åŠ é€Ÿæ¯”å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å…¶ä¸­ï¼Œæ¯ä¸ªç®—å­çš„åŠ é€Ÿæ¯”ç»¼åˆäº†å¤šä¸ªå½¢çŠ¶æµ‹ä¾‹çš„æ•°æ®ï¼Œä»£è¡¨è¯¥ç®—å­çš„æ•´ä½“æ€§èƒ½ã€‚

![ç®—å­åŠ é€Ÿæ¯”](./docs/assets/speedup-20251225.png)

## è´¡çŒ®ä»£ç 

æ¬¢è¿å¤§å®¶å‚ä¸ FlagGems çš„ç®—å­å¼€å‘å¹¶è´¡çŒ®ä»£ç ï¼Œè¯¦æƒ…è¯·å‚è€ƒ[CONTRIBUTING.md](./CONTRIBUTING_cn.md)ã€‚

## å¼•ç”¨

æ¬¢è¿å¼•ç”¨æˆ‘ä»¬çš„é¡¹ç›®ï¼š

```bibtex
@misc{flaggems2024,
    title={FlagOpen/FlagGems: FlagGems is an operator library for large language models implemented in the Triton language.},
    url={https://github.com/FlagOpen/FlagGems},
    journal={GitHub},
    author={BAAI FlagOpen team},
    year={2024}
}
```

## è”ç³»æˆ‘ä»¬

å¦‚æœ‰ç–‘é—®ï¼Œè¯·æäº¤ issueï¼Œæˆ–å‘é€é‚®ä»¶è‡³<a href="mailto:contact@flagos.io">contact@flagos.io</a>ã€‚

## è¯ä¹¦

æœ¬é¡¹ç›®åŸºäº[Apache 2.0](./LICENSE)ã€‚
